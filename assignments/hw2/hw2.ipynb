{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Homework 2:** The Hunt for the USS Scorpion (Part II)\n",
        "## Logistic regression, beta-binomial updating, and empirical Bayes\n",
        "STATS348, UChicago, Spring 2023\n",
        "\n",
        "----------------\n",
        "**Your name here:**\n",
        "\n",
        "----------------\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/aschein/stat_348/blob/main/assignments/hw2/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instructions\n",
        "\n",
        "The purpose of this homework is to apply the concepts raised in [lecture 3](https://github.com/aschein/stat_348/blob/main/materials/3_logreg_and_beta_binomial.pdf):\n",
        "\n",
        "* logistic regression\n",
        "* overfitting\n",
        "* beta-Binomial conjugacy\n",
        "* empirical Bayes\n",
        "\n",
        "This homework will also familiarize you with the Python package [scikit-learn](https://scikit-learn.org/stable/).\n",
        "\n",
        "Assignment is due **Monday April 3, 11:59pm** on Canvas."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting\n",
        "\n",
        "It is May 1968 and the USS _Scorpion_ has just disappeared somewhere in the Atlantic Ocean, likely off the coast of Spain. You are the lone statistician on board the USS _Mizar_, which has been dispatched to find the missing submarine. Your job is to guide the search as best you can, given the data at your disposal.\n",
        "\n",
        "## Search effectiveness probability (SEP)\n",
        "As we saw last week, an important component of our decision problem is the _search effectiveness probability_ of each search cell $k$\n",
        "\n",
        "\\begin{equation}\n",
        "q_k = P(\\textrm{finding the sub in $k$} \\mid \\textrm{sub is in $k$}, \\textrm{divers search $k$})\n",
        "\\end{equation}\n",
        "\n",
        "There are a number of factors that go into the SEP. The ocean floor is deeper in some cells, the ocean current is stronger, the water is murkier, etc. It's important we understand how easy it would be to actually detect the submarine in each cell before we send divers to look for it.\n",
        "\n",
        "In this assignment, we will explore two different ways of modeling $q_k$\n",
        "* Supervised learning, with logistic regression\n",
        "* Beta-binomial updating, and empirical Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# library for dataframes\n",
        "import pandas as pd\n",
        "\n",
        "# scientific computing libraries\n",
        "import numpy as np\n",
        "import numpy.random as rn\n",
        "import scipy.stats as st\n",
        "import scipy.special as sp\n",
        "\n",
        "# plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# progress bars for loops\n",
        "from tqdm.notebook import tqdm\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part I: Learning SEPs with logistic regression\n",
        "\n",
        "## Setting\n",
        "\n",
        "In this part of the assignment, we will define SEPs across cells to be a function of covariates\n",
        "\n",
        "\\begin{equation}\n",
        "q_k \\triangleq q(\\boldsymbol{x}_k)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\boldsymbol{x}_k \\in \\mathbb{R}^p$ are covariates associated with each search cell that measure aspects which relate to divers' ability to find things (e.g., seafloor depth, strength of current, etc.), and perhaps polynomial expansions and interactions between those base covariates.\n",
        "\n",
        "In this section, we will work with **binary trial data**. In each of $K=100$ cells, we have dropped one large object and seen whether divers were able to recover it. Define the outcome to be\n",
        "\n",
        "\\begin{align}\n",
        "y_k &= \\begin{cases}\n",
        "1& \\textrm{ if divers recovered the object}\\\\\n",
        "0& \\textrm{ otherwise}\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "We will work with **logistic regression** models that make the following assumption:\n",
        "\n",
        "\\begin{align}\n",
        "q_k \\equiv P(y_k = 1 \\mid \\boldsymbol{x}_k) = \\frac{\\exp{\\beta_0 + \\boldsymbol{\\beta}_1^\\top \\boldsymbol{x}_k}}{1 + \\exp{\\beta_0 + \\boldsymbol{\\beta}_1^\\top \\boldsymbol{x}_k}}\n",
        "\\end{align}\n",
        "\n",
        "Or, equivalently that\n",
        "\\begin{align}\n",
        "\\log (\\tfrac{q_k}{1-q_k}) = \\beta_0 + \\boldsymbol{\\beta}_1^\\top \\boldsymbol{x}_k\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read in the trial data\n",
        "Run the following cell, and explore the trial data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cell_id</th>\n",
              "      <th>seafloor_depth</th>\n",
              "      <th>water_temperature</th>\n",
              "      <th>strength_of_current</th>\n",
              "      <th>seafloor_composition</th>\n",
              "      <th>outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3581.873017</td>\n",
              "      <td>21.006783</td>\n",
              "      <td>moderate</td>\n",
              "      <td>sand</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>715.353815</td>\n",
              "      <td>11.553155</td>\n",
              "      <td>strong</td>\n",
              "      <td>rock</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2805.869109</td>\n",
              "      <td>10.707339</td>\n",
              "      <td>moderate</td>\n",
              "      <td>sand</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1154.116879</td>\n",
              "      <td>16.352951</td>\n",
              "      <td>strong</td>\n",
              "      <td>mud</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1363.089831</td>\n",
              "      <td>12.815914</td>\n",
              "      <td>moderate</td>\n",
              "      <td>mud</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>95</td>\n",
              "      <td>2659.833294</td>\n",
              "      <td>13.119218</td>\n",
              "      <td>strong</td>\n",
              "      <td>sand</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>96</td>\n",
              "      <td>1032.545545</td>\n",
              "      <td>15.699824</td>\n",
              "      <td>weak</td>\n",
              "      <td>sand</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>97</td>\n",
              "      <td>3883.862643</td>\n",
              "      <td>17.163361</td>\n",
              "      <td>moderate</td>\n",
              "      <td>mud</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>98</td>\n",
              "      <td>504.397526</td>\n",
              "      <td>10.698066</td>\n",
              "      <td>weak</td>\n",
              "      <td>rock</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>99</td>\n",
              "      <td>2564.698389</td>\n",
              "      <td>18.361401</td>\n",
              "      <td>weak</td>\n",
              "      <td>sand</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    cell_id  seafloor_depth  water_temperature strength_of_current  \\\n",
              "0         0     3581.873017          21.006783            moderate   \n",
              "1         1      715.353815          11.553155              strong   \n",
              "2         2     2805.869109          10.707339            moderate   \n",
              "3         3     1154.116879          16.352951              strong   \n",
              "4         4     1363.089831          12.815914            moderate   \n",
              "..      ...             ...                ...                 ...   \n",
              "95       95     2659.833294          13.119218              strong   \n",
              "96       96     1032.545545          15.699824                weak   \n",
              "97       97     3883.862643          17.163361            moderate   \n",
              "98       98      504.397526          10.698066                weak   \n",
              "99       99     2564.698389          18.361401                weak   \n",
              "\n",
              "   seafloor_composition  outcome  \n",
              "0                  sand        1  \n",
              "1                  rock        1  \n",
              "2                  sand        1  \n",
              "3                   mud        0  \n",
              "4                   mud        1  \n",
              "..                  ...      ...  \n",
              "95                 sand        1  \n",
              "96                 sand        0  \n",
              "97                  mud        0  \n",
              "98                 rock        1  \n",
              "99                 sand        0  \n",
              "\n",
              "[100 rows x 6 columns]"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_logreg_trials = pd.read_csv('logreg_trial_cells.csv')\n",
        "df_logreg_trials"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learn how to preprocess features with `scikit-learn`\n",
        "\n",
        "Review the code in following cell carefully where we have provided code that takes a `Pandas` dataframe, and turns it into a `numpy` array corresponding to a scaled (samples $\\times$ features)-covariate matrix $X$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "def get_preprocessor(continuous_cols, categorical_cols, ordinal_cols, ordinal_categories):\n",
        "    \"\"\" Returns a preprocessor object that can be used in a scikit-learn Pipeline.\n",
        "\n",
        "    Args:\n",
        "        continuous_cols (list): list of column names for continuous features\n",
        "        categorical_cols (list): list of column names for categorical features\n",
        "        ordinal_cols (list): list of column names for ordinal features\n",
        "        ordinal_categories (list): list of lists of categories for ordinal features\n",
        "    \n",
        "    Returns:\n",
        "        preprocessor (ColumnTransformer): preprocessor object that can be used in a scikit-learn Pipeline\n",
        "    \"\"\"\n",
        "    continuous_transformer = Pipeline(steps=[\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    \n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "    \n",
        "    ordinal_transformer = Pipeline(steps=[\n",
        "        ('ordinal', OrdinalEncoder(categories=ordinal_categories))\n",
        "    ])\n",
        "    \n",
        "    # Combine transformers into a single ColumnTransformer\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('continuous', continuous_transformer, continuous_cols),\n",
        "        ('categorical', categorical_transformer, categorical_cols),\n",
        "        ('ordinal', ordinal_transformer, ordinal_cols)\n",
        "    ])\n",
        "    \n",
        "    return preprocessor\n",
        "\n",
        "continuous_cols = ['seafloor_depth', 'water_temperature']\n",
        "categorical_cols = ['seafloor_composition']\n",
        "ordinal_cols = ['strength_of_current']\n",
        "ordinal_categories = [['weak', 'moderate', 'strong']]\n",
        "\n",
        "preprocessor = get_preprocessor(continuous_cols, categorical_cols, ordinal_cols, ordinal_categories)\n",
        "\n",
        "X = preprocessor.fit_transform(df_logreg_trials)\n",
        "\n",
        "print(\"Shape of covariate matrix X:\", X.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1 [Code review, write] (5pts)\n",
        "1) Explain why the shape of the covariate matrix above is (100, 6). Specifically, what do the 6 features correspond to? \n",
        "\n",
        "___\n",
        "\n",
        "Your answer here:\n",
        "\n",
        "___"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learn more about the scikit-learn `Pipeline`\n",
        "\n",
        "The last cell used the scikit-learn `Pipeline` to define a preprocessor that takes in a dataframe and outputs covariate matrix. In the following cell, we provide code that creates a more sophisticated `Pipeline`, which further accepts a desired degree of polynomial expansion and again outputs a covariate matrix. Study the code carefully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def get_pipeline(preprocessor, poly_degree=1):\n",
        "    \"\"\" Returns a scikit-learn Pipeline object consisting of a preprocessor and polynomial feature transformer.\n",
        "    \n",
        "    Args:\n",
        "        preprocessor (ColumnTransformer): preprocessor object that can be used in a scikit-learn Pipeline\n",
        "        poly_degree (int): degree of polynomial features to generate\n",
        "    \"\"\"\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('polynomial', PolynomialFeatures(degree=poly_degree, include_bias=True))\n",
        "    ])\n",
        "    return pipeline\n",
        "\n",
        "pipeline = get_pipeline(preprocessor, poly_degree=2)\n",
        "X = pipeline.fit_transform(df_logreg_trials)\n",
        "print(X.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2 [Code review, write] (10pts)\n",
        "1) Explain why the shape of the covariate matrix above is (100, 28). Specifically, what do the 28 features correspond to? In this case, do not list the name of each feature. Instead, provide a concise mathematical description of why there are 28 features.\n",
        "\n",
        "___\n",
        "\n",
        "Your answer here:\n",
        "\n",
        "___"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learn to use `LogisticRegression` in scikit-learn\n",
        "Study the following code carefully. At a high-level this code:\n",
        "\n",
        "* Preprocesses the data into two `numpy` arrays: $\\boldsymbol{y}, \\boldsymbol{X}$ \n",
        "* Defines a scikit-learn `LogisticRegression` model with no regularization\n",
        "* Fits the model to the data and computes the training **accuracy** and **negative log loss**\n",
        "* Defines a scikit-learn `KFold` object for cross-validation\n",
        "* Computes the average cross validation accuracy and negative log loss across the K folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "\n",
        "# binary outcomes\n",
        "y = df_logreg_trial_cells['outcome'].values\n",
        "\n",
        "# degree-5 polynomial features\n",
        "degree = 5\n",
        "pipeline = get_pipeline(preprocessor, poly_degree=degree)\n",
        "X = pipeline.fit_transform(df_logreg_trial_cells)\n",
        "\n",
        "# define logreg model with no regularization, saga solver, and 10,000 iterations\n",
        "logreg = LogisticRegression(penalty='none', solver='saga', max_iter=10000)\n",
        "\n",
        "# fit model to data\n",
        "logreg.fit(X, y)\n",
        "\n",
        "# calculate accuracy on training data\n",
        "y_pred = logreg.predict(X)\n",
        "train_accuracy = accuracy_score(y, y_pred)\n",
        "\n",
        "# calculate negative log loss on training data\n",
        "y_prob = logreg.predict_proba(X)\n",
        "train_neg_log_loss = -log_loss(y, y_prob)\n",
        "\n",
        "print('Training accuracy: {:.2f}'.format(train_accuracy))\n",
        "print('Training negative log loss: {:.2f}'.format(train_neg_log_loss))\n",
        "\n",
        "# define k-fold cross-validation\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "# calculate average cross-validation accuracy across K folds\n",
        "avg_kfold_accuracy = cross_val_score(logreg, X, y, cv=kfold, scoring='accuracy', n_jobs=-1).mean()\n",
        "print('Average k-fold accuracy: {:.2f}'.format(avg_kfold_accuracy))\n",
        "\n",
        "# calculate average cross-validation negative log loss across K folds\n",
        "avg_kfold_neg_log_loss = cross_val_score(logreg, X, y, cv=kfold, scoring='neg_log_loss', n_jobs=-1).mean()\n",
        "print('Average k-fold negative log loss: {:.2f}'.format(avg_kfold_neg_log_loss))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3 [Code review, write, math] (10pts)\n",
        "1) Explain why there might be a difference between the training versus cross validation accuracy (or negative log loss).\n",
        "\n",
        "___\n",
        "\n",
        "Your answer here:\n",
        "\n",
        "___\n",
        "\n",
        "2) Write in mathematical form exactly what `-log_loss(y, y_prob)` computes. Your equation should be in terms of the SEPs $q_k$.\n",
        "___\n",
        "\n",
        "Your answer here:\n",
        "\n",
        "___"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 4 [Code] (10pts)\n",
        "\n",
        "Using what you have learned about how to preprocess data, fit a log-reg model, and use K-fold cross validation...\n",
        "\n",
        "1) Write code in the following cell that loops through polynomial degrees 1...5, and for each:\n",
        "    * Prepocesses the data using polynomial feature expansions of the given degree\n",
        "    * Fits a single unregularized logistic regression model\n",
        "    * Calculates and saves its training accuracy and negative log loss\n",
        "    * Creates a K-fold cross validation object\n",
        "    * Calculates and saves the average cross-validation accuracy and negative log loss across K folds\n",
        "\n",
        " \n",
        "2) Plot the results\n",
        "    * A plot depicting training versus average cross-validation **accuracy (y-axis)** as a function of **polynomial degree (x-axis)**\n",
        "    * A plot depicting training versus average cross-validation **negative log loss (y-axis)** as a function of **polynomial degree (x-axis)**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the range of polynomial degrees to test\n",
        "degrees = list(range(1, 6))\n",
        "\n",
        "# iterate through each degree, preprocess X, fit model and K-fold CV, calculate training vs kfold accuracy and neg_log_loss\n",
        "for degree in tqdm(degrees):\n",
        "    \n",
        "    # Your code here\n",
        "\n",
        "# Plot the results: training versus k-fold CV accuracy (y-axis) as a function of degree (x-axis)\n",
        "\n",
        "# Your code here\n",
        "\n",
        "# Plot the results: training versus k-fold CV negative log-loss (y-axis) as a function of degree (x-axis)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 5 [Write] (5pts)\n",
        "Reflect on the above plots you have generated. What are some conclusions you might draw from them?\n",
        "\n",
        "___\n",
        "\n",
        "Your answer here:\n",
        "\n",
        "___"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learn to use `GridSearchCV`\n",
        "In the following cell, we have provided code that uses `GridSearchCV` to perform a grid search over the best-performing combination of \n",
        "* polynomial degree (`degree` = [1...5]) $\\times$\n",
        "* regularization type (`l1` versus `l2`) $\\times$\n",
        "* regularization strength (`C`=[0.1...5])\n",
        "\n",
        "The code finds the logreg model with the lowest cross-validation log loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create a pipeline for the whole process\n",
        "pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('polynomial', PolynomialFeatures(include_bias=True)),\n",
        "        ('classifier', LogisticRegression(solver='saga', max_iter=10000))\n",
        "    ])\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'polynomial__degree': [1, 2, 3, 4, 5],\n",
        "    'classifier__C': [0.1, 0.25, 0.5, 0.75, 1, 2, 5],\n",
        "    'classifier__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to find the best combination of parameters and fit the data\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_log_loss', refit=True)\n",
        "grid_search.fit(df_logreg_trial_cells, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best CV negative log loss: \", grid_search.best_score_)\n",
        "\n",
        "# Save the best model as best_model\n",
        "best_model = grid_search.best_estimator_"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 6 [Code] (10pts)\n",
        "In this question, you will use the results of grid search above. In the cell below, write code that:\n",
        "\n",
        "1) Uses the best-performing model to calculate the SEPs for all trial cells.\n",
        "\n",
        "2) Plots a histogram of the estimated SEPs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the best CV model to calculate the SEPs for all trial cells\n",
        "\n",
        "# Your code here \n",
        "\n",
        "# Plot a histogram of the SEPS\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part II: Beta-binomial updating and empirical Bayes\n",
        "\n",
        "In this second part, we will approach the problem differently. Instead of using covariates to share information across cells, we will assume that all SEPs come from a **shared prior distribution** $q_k \\stackrel{\\textrm{iid}}{\\sim}P(q)$. \n",
        "\n",
        "This part also incorporates the possibility of multiple trials $N_k$ per cell. The outcome data is now\n",
        "\\begin{align}\n",
        "y_k &\\in \\{0,\\dots, N_k\\}&\\textrm{ the number of successful trials}\n",
        "\\end{align}\n",
        "\n",
        "We will assume the following beta-binomial model:\n",
        "\n",
        "\\begin{align}\n",
        "q_k &\\stackrel{\\textrm{iid}}{\\sim} \\textrm{Beta}(a_0,\\,b_0) \\\\\n",
        "y_k &\\stackrel{\\textrm{ind.}}{\\sim} \\textrm{Binom}(N_k,\\, q_k)\n",
        "\\end{align}\n",
        "\n",
        "In this model, we assume the SEPs $q_k$ across all cells $k$ were drawn from a shared beta distribution with shape parameters $a_0$ and $b_0$. The number of successes $y_k$ conditioned on the number of trials $N_k$ and $q_k$ is then binomially-distributed.\n",
        "\n",
        "Put another way, this implies the following **marginal likelihood** for the trial data:\n",
        "\n",
        "\\begin{align}\n",
        "P(\\boldsymbol{y}_{1:K} \\mid \\boldsymbol{N}_{1:K}, a_0,\\,b_0\\,) &= \\prod_{k=1}^K P(y_k \\mid N_k, a_0,\\,b_0\\,) \\\\\n",
        "&= \\prod_{k=1}^K \\int \\underbrace{P(y_k \\mid N_k,\\, q_k)}_{\\textrm{Binom}(y_k; N_k\\,q_k)} \\, \\underbrace{P(q_k \\mid a_0,\\, b_0)}_{\\textrm{Beta}(q_k;a_0,\\,b_0)}\\,dq_k\n",
        "\\end{align}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 7 [Math] (10pts)\n",
        "1) Provide an analytic expression (i.e., without any integrals) for the negative log marginal likelihood:\n",
        "\\begin{equation}\n",
        "-\\log P(\\boldsymbol{y}_{1:K} \\mid \\boldsymbol{N}_{1:K}, a_0,\\,b_0\\,) = \\textrm{ Your answer here }\n",
        "\\end{equation}\n",
        "\n",
        "2) Briefly explain how you came to this expression.\n",
        "\n",
        "___\n",
        "\n",
        "Your answer here:\n",
        "\n",
        "___"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 8 [Code] (15pts)\n",
        "\n",
        "In this question, you will learn to use `scipy.optimize.minimize` to minimize a custom function. Make sure to read the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) and to experiment with different settings.\n",
        "\n",
        "Write the following code in the cell below:\n",
        "\n",
        "1) Implement a function to compute the negative log marginal likelihood. Your function can use **any** functions available in the `numpy` or `scipy` Python libraries.\n",
        "\n",
        "\n",
        "2) Implement a function that estimates the parameters $a_0, b_0$ using type-II maximum likelihood by minimizing your function `neg_log_marginal_likelihood` using `scipy.optimize.minimize`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "def neg_log_marginal_likelihood(params, y, n):\n",
        "    \"\"\" Calculate the negative log-marginal likelihood for the beta-binomial model.\n",
        "    \n",
        "    Args:\n",
        "        params (tuple): the parameters of the marginal likelihood (a0, b0)\n",
        "        y (array): the number of successes for each trial\n",
        "        n (array): the number of trials\n",
        "    \n",
        "    Returns:\n",
        "        float: the negative log-marginal likelihood\n",
        "    \"\"\"\n",
        "    a0, b0 = params\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "    return neg_log_marginal_likelihood\n",
        "\n",
        "def fit_marginal_likelihood(y, n):\n",
        "    \"\"\" Fit the parameters of the marginal likelihood to the data using maximum likelihood.\n",
        "\n",
        "    Uses scipy.optimize.minimize and calls neg_log_marginal_likelihood.\n",
        "\n",
        "    Args:\n",
        "        y (array): the number of successes for each trial\n",
        "        n (array): the number of trials\n",
        "        \n",
        "        If you add other args, describe them here.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: the MLE for a0 and b0\n",
        "    \"\"\"\n",
        "    \n",
        "    # Your code here\n",
        "\n",
        "    return a0_mle, b0_mle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run your code on the trial data\n",
        "\n",
        "Run the following code, which:\n",
        "1) Loads the trial data. Note that this time, we have multiple trials per cell $N_k$ (`n_trials`) and potentially multiple successes per cell $y_k$ (`n_successes`)\n",
        "\n",
        "2) Runs your `fit_marginal_likelihood` function on the data. Make sure to modify this line if your function takes additional arguments\n",
        "\n",
        "3) Plots the estimated **empirical prior**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load in the beta-binomial trial data\n",
        "df_trials = pd.read_csv('betabin_trial_cells.csv')\n",
        "y = df_trials['n_successes'].values\n",
        "n = df_trials['n_trials'].values\n",
        "\n",
        "# if your fit_marginal_likelihood function takes extra args, modify this code to pass them in here.\n",
        "a0_mle, b0_mle = fit_marginal_likelihood(y, n)\n",
        "print(a0_mle, b0_mle)\n",
        "\n",
        "# plot the empirical prior beta distribution\n",
        "x_axis_vals = np.linspace(0, 1, 100)\n",
        "y_axis_vals = st.beta.pdf(x_axis_vals, a0_mle, b0_mle)\n",
        "_ = plt.plot(x_axis_vals, y_axis_vals, color='Green')\n",
        "_ = plt.fill_between(x_axis_vals, y_axis_vals, alpha=0.2, color='Green')\n",
        "_ = plt.title('Empirical Prior Beta Distribution')\n",
        "_ = plt.xlabel('SEP')\n",
        "_ = plt.ylabel('density')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 9 [Math] (10pts)\n",
        "\n",
        "1) Provide the form of the posterior over cell $k$'s SEP $q_k$, conditioned on the number of trials $N_k$ and successes $y_k$ in the cell, and the estimated parameters $\\hat{a_0}$, $\\hat{b_0}$:\n",
        "\\begin{equation}\n",
        "P(q_k \\mid y_k,\\, N_k,\\,\\hat{a_0}, \\hat{b_0}) = \\textrm{Your answer here}\n",
        "\\end{equation}\n",
        "\n",
        "2) Provide the form of the posterior expectation\n",
        "\\begin{equation}\n",
        "E[q_k \\mid y_k,\\, N_k,\\,\\hat{a_0}, \\hat{b_0}] = \\textrm{Your answer here}\n",
        "\\end{equation}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 10 [Code] (5pts)\n",
        "1) Implement a function to compute the posterior expectation above.\n",
        "\n",
        "2) Compute the posterior expectation of $q_k$ for all trial cells.\n",
        "\n",
        "3) Plot a histogram of the estimated SEPs. (Run the code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "def posterior_expectation(y, n, a0, b0):\n",
        "    \"\"\" Calculate the posterior expectation of SEP.\n",
        "\n",
        "    Args:\n",
        "        y (array): the number of successes for each trial\n",
        "        n (array): the number of trials\n",
        "        a0 (float): the prior alpha parameter\n",
        "        b0 (float): the prior beta parameter\n",
        "    \n",
        "    Returns:\n",
        "        float: the posterior expectation of SEP\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "\n",
        "    return estimated_SEP\n",
        "\n",
        "estimated_SEP = posterior_expectation(y, n, a0_mle, b0_mle)\n",
        "_ = plt.hist(estimated_SEP)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question [Write] (10pts) \n",
        "\n",
        "1) Reflect on the pros and cons of the two approaches we used to estimate SEPs (supervised learning and posterior updating). Under what conditions might we  prefer one over the other?\n",
        "\n",
        "___\n",
        "\n",
        "Your answer here:\n",
        "\n",
        "___\n",
        "\n",
        "2) Reflect on how we might combine these approaches. Describe a third approach that would combine elements of both:\n",
        "\n",
        "___\n",
        "\n",
        "Your answer here:\n",
        "\n",
        "___\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOEIcgdHfyD4yGF2Q4D7WMS",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "e31d8529818713d98a38061e476a303cb7aebfe46db2981fe05de46bbfa528ef"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
